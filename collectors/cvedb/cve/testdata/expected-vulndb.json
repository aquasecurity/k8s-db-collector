[{"id":"CVE-2023-2431","created_at":"2023-06-15T14:42:32Z","summary":"Bypass of seccomp profile enforcement ","component":"github.com/kubernetes/kubelet","description":"What happened?A security issue was discovered in Kubelet that allows pods to bypass the seccomp profile enforcement. This issue has been rated LOW (CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:U/C:L/I:L/A:N) (score: 3.4).If you have pods in your cluster that use localhost type for seccomp profile but specify an empty profile field, then you are affected by this issue. In this scenario, this vulnerability allows the pod to run in “unconfined” (seccomp disabled) mode. This bug affects Kubelet.How can we reproduce it (as minimally and precisely as possible)?This can be reproduced by creating a pod with following sample seccomp Localhost profile -https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#seccompprofile-v1-coreKubernetes version","affected_version":[{"from":"1.27.0","to":"1.27.1"},{"from":"1.26.0","to":"1.26.4"},{"from":"1.25.0","to":"1.25.9"},{"from":"0.0.0","to":"1.24.13"}],"fixed_version":[{"fixed":"1.27.2"},{"fixed":"1.26.5"},{"fixed":"1.25.10"},{"fixed":"1.24.14"}],"urls":["https://github.com/kubernetes/kubernetes/issues/118690","https://www.cve.org/cverecord?id=CVE-2023-2431"],"cvss":"CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:U/C:L/I:L/A:N","severity":"Low","score":3.4},{"id":"CVE-2023-2727, CVE-2023-2728","created_at":"2023-06-13T14:42:06Z","summary":"Bypassing policies imposed by the ImagePolicyWebhook and bypassing mountable secrets policy imposed by the ServiceAccount admission plugin","component":"github.com/kubernetes/kube-apiserver","description":"CVE-2023-2727: Bypassing policies imposed by the ImagePolicyWebhook admission pluginCVSS Rating: CVSS:3.1/AV:N/AC:L/PR:H/UI:N/S:U/C:H/I:H/A:NA security issue was discovered in Kubernetes where users may be able to launch containers using images that are restricted by ImagePolicyWebhook when using ephemeral containers. Kubernetes clusters are only affected if the ImagePolicyWebhook admission plugin is used together with ephemeral containers.Am I vulnerable?Clusters are impacted by this vulnerability if all of the following are true:","affected_version":[{"from":"1.27.0","to":"1.27.2"},{"from":"1.26.0","to":"1.26.5"},{"from":"1.25.0","to":"1.25.10"},{"from":"0.0.0","to":"1.24.14"}],"fixed_version":[{"fixed":"1.27.3"},{"fixed":"1.26.6"},{"fixed":"1.25.11"},{"fixed":"1.24.15"}],"urls":["https://github.com/kubernetes/kubernetes/issues/118640","https://www.cve.org/cverecord?id=CVE-2023-2727, CVE-2023-2728"],"cvss":"CVSS:3.1/AV:N/AC:L/PR:H/UI:N/S:U/C:H/I:H/A:N","severity":"Medium","score":6.5},{"id":"CVE-2023-2878","created_at":"2023-06-02T19:03:54Z","summary":"secrets-store-csi-driver discloses service account tokens in logs","component":"github.com/kubernetes/secrets-store-csi-driver","description":"A security issue was discovered in secrets-store-csi-driver where an actor with access to the driver logs could observe service account tokens. These tokens could then potentially be exchanged with external cloud providers to access secrets stored in cloud vault solutions. Tokens are only logged when TokenRequests is configured in the CSIDriver object and the driver is set to run at log level 2 or greater via the -v flag.This issue has been rated MEDIUM CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:N/A:N (6.5), and assigned CVE-2023-2878Am I vulnerable?You may be vulnerable if TokenRequests is configured in the CSIDriver object and the driver is set to run at log level 2 or greater via the -v flag.To check if token requests are configured, run the following command:To check if tokens are being logged, examine the secrets-store container log:","affected_version":[{"from":"0.0.0","to":"1.3.3"}],"fixed_version":[{"fixed":"1.3.3"}],"urls":["https://github.com/kubernetes/kubernetes/issues/118419","https://www.cve.org/cverecord?id=CVE-2023-2878"],"cvss":"CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:N/A:N","severity":"Medium","score":6.5},{"id":"CVE-2022-3294","created_at":"2022-11-08T21:33:26Z","summary":"Node address isn't always verified when proxying","component":"github.com/kubernetes/kube-apiserver","description":"CVSS Rating: CVSS:3.1/AV:N/AC:H/PR:H/UI:N/S:U/C:H/I:H/A:HA security issue was discovered in Kubernetes where users may have access to secure endpoints in the control plane network. Kubernetes clusters are only affected if an untrusted user can modify Node objects and send proxy requests to them.Kubernetes supports node proxying, which allows clients of kube-apiserver to access endpoints of a Kubelet to establish connections to Pods, retrieve container logs, and more. While Kubernetes already validates the proxying address for Nodes, a bug in kube-apiserver made it possible to bypass this validation. Bypassing this validation could allow authenticated requests destined for Nodes to to the API server's private network.Am I vulnerable?Clusters are affected by this vulnerability if there are endpoints that the kube-apiserver has connectivity to that users should not be able to access. This includes:mTLS services that accept the same client certificate as nodes may be affected. The severity of this issue depends on the privileges \u0026 sensitivity of the exploitable endpoints.Clusters that configure the egress selector to use a proxy for cluster traffic may not be affected.","affected_version":[{"from":"0.0.0","to":"1.25.3"},{"from":"0.0.0","to":"1.24.7"},{"from":"0.0.0","to":"1.23.13"},{"from":"0.0.0","to":"1.22.15"}],"fixed_version":[{"fixed":"1.25.4"},{"fixed":"1.24.8"},{"fixed":"1.23.14"},{"fixed":"1.22.16"}],"urls":["https://github.com/kubernetes/kubernetes/issues/113757","https://www.cve.org/cverecord?id=CVE-2022-3294"],"cvss":"CVSS:3.1/AV:N/AC:H/PR:H/UI:N/S:U/C:H/I:H/A:H","severity":"Medium","score":6.6},{"id":"CVE-2022-3162","created_at":"2022-11-08T21:33:07Z","summary":"Unauthorized read of Custom Resources","component":"github.com/kubernetes/kube-apiserver","description":"CVSS Rating: CVSS:3.0/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:NA security issue was discovered in Kubernetes where users authorized to list or watch one type of namespaced custom resource cluster-wide can read custom resources of a different type in the same API group without authorization.Am I vulnerable?Clusters are impacted by this vulnerability if all of the following are true:","affected_version":[{"from":"0.0.0","to":"1.25.3"},{"from":"0.0.0","to":"1.24.7"},{"from":"0.0.0","to":"1.23.13"},{"from":"0.0.0","to":"1.22.15"}],"fixed_version":[{"fixed":"1.25.4"},{"fixed":"1.24.8"},{"fixed":"1.23.14"},{"fixed":"1.22.16"}],"urls":["https://github.com/kubernetes/kubernetes/issues/113756","https://www.cve.org/cverecord?id=CVE-2022-3162"],"cvss":"CVSS:3.0/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N","severity":"Medium","score":6.5},{"id":"CVE-2022-3172","created_at":"2022-09-16T13:14:50Z","summary":"Aggregated API server can cause clients to be redirected (SSRF)","component":"github.com/kubernetes/kube-apiserver","description":"CVSS Rating: CVSS:3.1/AV:N/AC:H/PR:H/UI:R/S:C/C:L/I:L/A:L (5.1, medium)A security issue was discovered in kube-apiserver that allows an aggregated API server to redirect client traffic to any URL.  This could lead to the client performing unexpected actions as well as forwarding the client's API server credentials to third parties.This issue has been rated medium and assigned CVE-2022-3172Am I vulnerable?All Kubernetes clusters with the following versions that are running aggregated API servers are impacted.  To identify if you have aggregated API servers configured, run the following command:","affected_version":[{"from":"1.25.0","to":"1.25.0"},{"from":"1.24.0","to":"1.24.4"},{"from":"1.23.0","to":"1.23.10"},{"from":"1.22.0","to":"1.22.13"},{"from":"0.0.0","to":"1.21.14"}],"fixed_version":[{"fixed":"1.25.1"},{"fixed":"1.24.5"},{"fixed":"1.23.11"},{"fixed":"1.22.14"}],"urls":["https://github.com/kubernetes/kubernetes/issues/112513","https://www.cve.org/cverecord?id=CVE-2022-3172"],"cvss":"CVSS:3.1/AV:N/AC:H/PR:H/UI:R/S:C/C:L/I:L/A:L","severity":"Medium","score":5.1},{"id":"CVE-2021-25749","created_at":"2022-09-01T21:02:01Z","summary":"`runAsNonRoot` logic bypass for Windows containers","component":"github.com/kubernetes/kubelet","description":"A security issue was discovered in Kubernetes that could allow  Windows workloads to run as ContainerAdministrator even when those workloads set the runAsNonRoot option to true.This issue has been rated low and assigned CVE-2021-25749Am I vulnerable?All Kubernetes clusters with following versions, running Windows workloads with runAsNonRoot are impacted","affected_version":[{"from":"1.22.0","to":"1.22.13"},{"from":"1.23.0","to":"1.23.10"},{"from":"1.24.0","to":"1.24.4"}],"fixed_version":[{"fixed":"1.22.14"},{"fixed":"1.23.11"},{"fixed":"1.24.5"},{"fixed":"1.25.0"}],"urls":["https://github.com/kubernetes/kubernetes/issues/112192","https://www.cve.org/cverecord?id=CVE-2021-25749"]},{"id":"CVE-2021-25741","created_at":"2021-09-13T20:58:56Z","summary":"Symlink Exchange Can Allow Host Filesystem Access","component":"github.com/kubernetes/kubelet","description":"A security issue was discovered in Kubernetes where a user may be able to create a container with subpath volume mounts to access files \u0026 directories outside of the volume, including on the host filesystem.This issue has been rated High (CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H), and assigned CVE-2021-25741.Affected Components and ConfigurationsThis bug affects kubelet.Environments where cluster administrators have restricted the ability to create hostPath mounts are the most seriously affected. Exploitation allows hostPath-like access without use of the hostPath feature, thus bypassing the restriction.In a default Kubernetes environment, exploitation could be used to obscure misuse of already-granted privileges.","affected_version":[{"from":"1.22.0","to":"1.22.1"},{"from":"1.21.0","to":"1.21.4"},{"from":"1.20.0","to":"1.20.10"},{"from":"0.0.0","to":"1.19.14"}],"fixed_version":[{"fixed":"1.22.2"},{"fixed":"1.21.5"},{"fixed":"1.20.11"},{"fixed":"1.19.15"}],"urls":["https://github.com/kubernetes/kubernetes/issues/104980","https://www.cve.org/cverecord?id=CVE-2021-25741"],"cvss":"CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H","severity":"High","score":8.8},{"id":"CVE-2021-25737","created_at":"2021-05-18T19:14:27Z","summary":"Holes in EndpointSlice Validation Enable Host Network Hijack","component":"github.com/kubernetes/kube-apiserver","description":"Issue DetailsA security issue was discovered in Kubernetes where a user may be able to redirect pod traffic to private networks on a Node. Kubernetes already prevents creation of Endpoint IPs in the localhost or link-local range, but the same validation was not performed on EndpointSlice IPs. This issue has been rated Low (CVSS:3.0/AV:N/AC:L/PR:H/UI:N/S:U/C:L/I:N/A:N), and assigned CVE-2021-25737.Affected Componentkube-apiserver","affected_version":[{"from":"1.21.0","to":"1.21.0"},{"from":"1.20.0","to":"1.20.6"},{"from":"1.19.0","to":"1.19.10"},{"from":"1.16.0","to":"1.18.18"}],"fixed_version":[{"fixed":"1.21.1"},{"fixed":"1.20.7"},{"fixed":"1.19.11"},{"fixed":"1.18.19"}],"urls":["https://github.com/kubernetes/kubernetes/issues/102106","https://www.cve.org/cverecord?id=CVE-2021-25737"],"cvss":"CVSS:3.0/AV:N/AC:L/PR:H/UI:N/S:U/C:L/I:N/A:N","severity":"Low","score":2.7},{"id":"CVE-2021-3121","created_at":"2021-04-23T18:07:32Z","summary":"Processes may panic upon receipt of malicious protobuf messages","component":"github.com/kubernetes/","description":"Issue DetailsA security issue was discovered in code generated by the gogo protobuf compiler used by Kubernetes. The gogo protobuf compiler issue has been assigned CVE-2021-3121 and is also known as the “skippy peanut butter bug”.A program which uses affected code to handle a malicious protobuf message could panic.The Kubernetes Product Security Committee has tested the API server using a malicious message, and we believe that there is no security impact to Kubernetes.  When an authenticated user sent the malicious message to the API server, a panic occurred. However, the panic handler recovered and the API server continued without interruption (except to the malicious requestor, who received no response).Generated protobuf files are part of several Kubernetes repositories, and any downstream projects which vendor in these repos should evaluate whether there is any security impact to their project.Affected Components and ConfigurationsAny golang components which use handler code created by the gogo protbuf compiler, which accept protobuf messages and do not gracefully handle panics in the unmarshalling codepath may be affected.The following Linux command can be used to detect affected generated code within a codebase:Although we do not believe there is any security impact to Kubernetes, we have updated all generated protobufs out of an abundance of caution and as a courtesy to any downstream consumers who may be affected. The following PRs addressed this issue in Kubernetes:Master branch: #98477, #1013061.21 branch: #98477 (in 1.21.0), #101325 (in 1.21.1)1.20 branch: #100501 (in 1.20.6), #101326 (in 1.20.7)1.19 branch: #100515 (in 1.19.10), #101327 (in 1.19.11)1.18 branch: #100514 (in 1.18.18), #101335 (in 1.18.19)For other generated protobuf go handlers, the issue can be remediated by upgrading the gogo protobuf compiler to a fixed version (v1.3.2 or later), then regenerating affected protobuf code with the updated protobuf compiler.MitigationsDisabling support for protobuf messages may be one possible mitigation for any affected product.Also, graceful panic handling in message handlers mitigates the bug.DetectionIf you use generated protobuf code in a product and you observe a process exiting with messages similar to the following, a malicious user may be exploiting this defect:References","urls":["https://github.com/kubernetes/kubernetes/issues/101435","https://www.cve.org/cverecord?id=CVE-2021-3121"]},{"id":"CVE-2021-25735","created_at":"2021-03-10T18:18:01Z","summary":"Validating Admission Webhook does not observe some previous fields","component":"github.com/kubernetes/kube-apiserver","description":"A security issue was discovered in kube-apiserver that could allow node updates to bypass a Validating Admission Webhook. You are only affected by this vulnerability if you run a Validating Admission Webhook for Nodes that denies admission based at least partially on the old state of the Node object.This issue has been rated Medium (CVSS:3.0/AV:N/AC:L/PR:H/UI:N/S:U/C:N/I:H/A:H), and assigned CVE-2021-25735.Note: This only impacts validating admission plugins that rely on old values in certain fields, and does not impact calls from kubelets that go through the built-in NodeRestriction admission plugin.","affected_version":[{"from":"1.20.0","to":"1.20.5"},{"from":"1.19.0","to":"1.19.9"},{"from":"0.0.0","to":"1.18.17"}],"fixed_version":[{"fixed":"1.21.0"},{"fixed":"1.20.6"},{"fixed":"1.19.10"},{"fixed":"1.18.18"}],"urls":["https://github.com/kubernetes/kubernetes/issues/100096","https://www.cve.org/cverecord?id=CVE-2021-25735"],"cvss":"CVSS:3.0/AV:N/AC:L/PR:H/UI:N/S:U/C:N/I:H/A:H","severity":"Medium","score":6.5},{"id":"CVE-2020-8554","created_at":"2020-12-04T20:02:15Z","summary":"Man in the middle using LoadBalancer or ExternalIPs","component":"github.com/kubernetes/","description":"CVSS Rating: Medium (CVSS:3.0/AV:N/AC:L/PR:L/UI:N/S:U/C:L/I:L/A:L)This issue affects multitenant clusters. If a potential attacker can already create or edit services and pods, then they may be able to intercept traffic from other pods (or nodes) in the cluster.An attacker that is able to create a ClusterIP service and set the spec.externalIPs field can intercept traffic to that IP. An attacker that is able to patch the status (which is considered a privileged operation and should not typically be granted to users) of a LoadBalancer service can set the status.loadBalancer.ingress.ip to similar effect.This issue is a design flaw that cannot be mitigated without user-facing changes.Affected Components and ConfigurationsAll Kubernetes versions are affected. Multi-tenant clusters that grant tenants the ability to create and update services and pods are most vulnerable.MitigationsThere is no patch for this issue, and it can currently only be mitigated by restricting access to the vulnerable features. Because an in-tree fix would require a breaking change, we will open a conversation about a longer-term fix or built-in mitigation after the embargo is liftedTo restrict the use of external IPs we are providing an admission webhook container: k8s.gcr.io/multitenancy/externalip-webhook:v1.0.0. The source code and deployment instructions are published at https://github.com/kubernetes-sigs/externalip-webhook.Alternatively, external IPs can be restricted using OPA Gatekeeper. A sample ConstraintTemplate and Constraint can be found here: https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/general/externalip.No mitigations are provided for LoadBalancer IPs since we do not recommend granting users patch service/status permission. If LoadBalancer IP restrictions are required, the approach for the external IP mitigations can be copied.DetectionExternalIP services are not widely used, so we recommend manually auditing any external IP usage. Users should not patch service status, so audit events for patch service status requests authenticated to a user may be suspicious.If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.ioAcknowledgementsThis vulnerability was reported by Etienne Champetier (@champtar) of Anevia./area security/kind bug/committee product-security/sig network","urls":["https://github.com/kubernetes/kubernetes/issues/97076","https://www.cve.org/cverecord?id=CVE-2020-8554"],"cvss":"CVSS:3.0/AV:N/AC:L/PR:L/UI:N/S:U/C:L/I:L/A:L","severity":"Medium","score":6.3},{"id":"CVE-2020-8566","created_at":"2020-10-15T22:07:53Z","summary":"Ceph RBD adminSecrets exposed in logs when loglevel \u003e= 4","component":"github.com/kubernetes/kubernetes","description":"CVSS Rating: 4.7 CVSS:3.0/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:N/A:N (Medium)In Kubernetes clusters using Ceph RBD as a storage provisioner, with logging level of at least 4,  Ceph RBD admin secrets can be written to logs. This occurs in kube-controller-manager's logs during provisioning of Ceph RBD persistent claims.Am I vulnerable?If Ceph RBD volumes are in use and kube-controller-manager is using a log level of at least 4.","affected_version":[{"from":"1.19.0","to":"1.19.2"},{"from":"1.18.0","to":"1.18.9"},{"from":"1.17.0","to":"1.17.12"}],"fixed_version":[{"fixed":"1.19.3"},{"fixed":"1.18.10"},{"fixed":"1.17.13"}],"urls":["https://github.com/kubernetes/kubernetes/issues/95624","https://www.cve.org/cverecord?id=CVE-2020-8566"],"cvss":"CVSS:3.0/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:N/A:N","severity":"Medium","score":4.7},{"id":"CVE-2020-8565","created_at":"2020-10-15T22:05:32Z","summary":"Incomplete fix for CVE-2019-11250 allows for token leak in logs when logLevel \u003e= 9","component":"github.com/kubernetes/kubernetes","description":"CVSS Rating: 4.7 CVSS:3.0/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:N/A:N (Medium)In Kubernetes, if the logging level is to at least 9, authorization and bearer tokens will be written to log files. This can occur both in API server logs and client tool output like kubectl.Am I vulnerable?If kube-apiserver is using a log level of at least 9.","affected_version":[{"from":"1.19.0","to":"1.19.5"},{"from":"1.18.0","to":"1.18.13"},{"from":"1.17.0","to":"1.17.15"}],"fixed_version":[{"fixed":"1.20.0"},{"fixed":"1.19.6"},{"fixed":"1.18.14"},{"fixed":"1.17.16"}],"urls":["https://github.com/kubernetes/kubernetes/issues/95623","https://www.cve.org/cverecord?id=CVE-2020-8565"],"cvss":"CVSS:3.0/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:N/A:N","severity":"Medium","score":4.7},{"id":"CVE-2020-8564","created_at":"2020-10-15T22:03:19Z","summary":"Docker config secrets leaked when file is malformed and log level \u003e= 4","component":"github.com/kubernetes/kubernetes","description":"CVSS Rating: 4.7 CVSS:3.0/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:N/A:N (Medium)In Kubernetes clusters using a logging level of at least 4, processing a malformed docker config file will result in the contents of the docker config file being leaked, which can include pull secrets or other registry credentials.Am I vulnerable?If kubernetes.io/dockerconfigjson type secrets are used, and a log level of 4 or higher is used. Third party tools using k8s.io/kubernetes/pkg/credentialprovider to read docker config files may also be vulnerable.","affected_version":[{"from":"1.19.0","to":"1.19.2"},{"from":"1.18.0","to":"1.18.9"},{"from":"1.17.0","to":"1.17.12"}],"fixed_version":[{"fixed":"1.19.3"},{"fixed":"1.18.10"},{"fixed":"1.17.13"}],"urls":["https://github.com/kubernetes/kubernetes/issues/95622","https://www.cve.org/cverecord?id=CVE-2020-8564"],"cvss":"CVSS:3.0/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:N/A:N","severity":"Medium","score":4.7},{"id":"CVE-2020-8563","created_at":"2020-10-15T22:00:44Z","summary":"Secret leaks in kube-controller-manager when using vSphere provider","component":"github.com/kubernetes/kube-controller-manager","description":"CVSS Rating: 5.6 CVSS:3.0/AV:L/AC:H/PR:L/UI:N/S:C/C:H/I:N/A:N (Medium)In Kubernetes clusters using VSphere as a cloud provider, with a logging level set to 4 or above, VSphere cloud credentials will be leaked in the cloud controller manager's log.Am I vulnerable?If you are using VSphere as a cloud provider, have verbose logging enabled, and an attacker can access cluster logs, then you may be vulnerable to this.","affected_version":[{"from":"1.19.0","to":"1.19.2"}],"fixed_version":[{"fixed":"1.19.3"}],"urls":["https://github.com/kubernetes/kubernetes/issues/95621","https://www.cve.org/cverecord?id=CVE-2020-8563"],"cvss":"CVSS:3.0/AV:L/AC:H/PR:L/UI:N/S:C/C:H/I:N/A:N","severity":"Medium","score":5.6},{"id":"CVE-2020-8557","created_at":"2020-07-13T18:39:08Z","summary":"Node disk DOS by writing to container /etc/hosts","component":"github.com/kubernetes/kubelet","description":"CVSS Rating: Medium (5.5)  CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H/CR:H/IR:H/AR:MThe /etc/hosts file mounted in a pod by kubelet is not included by the kubelet eviction manager when calculating ephemeral storage usage by a pod. If a pod writes a large amount of data to the /etc/hosts file, it could fill the storage space of the node and cause the node to fail.Am I vulnerable?Any clusters allowing pods with sufficient privileges to write to their own /etc/hosts files are affected. This includes containers running with CAP_DAC_OVERRIDE in their capabilities bounding set (true by default) and either UID 0 (root) or a security context with allowPrivilegeEscalation: true (true by default).","affected_version":[{"from":"1.18.0","to":"1.18.5"},{"from":"1.17.0","to":"1.17.8"},{"from":"0.0.0","to":"1.16.13"}],"fixed_version":[{"fixed":"1.18.6"},{"fixed":"1.17.9"},{"fixed":"1.16.13"}],"urls":["https://github.com/kubernetes/kubernetes/issues/93032","https://www.cve.org/cverecord?id=CVE-2020-8557"],"cvss":"CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H","severity":"Medium","score":5.5},{"id":"CVE-2020-8559","created_at":"2020-07-08T17:03:16Z","summary":"Privilege escalation from compromised node to cluster","component":"github.com/kubernetes/kube-apiserver","description":"CVSS Rating: Medium (6.4) CVSS:3.1/AV:N/AC:H/PR:H/UI:R/S:U/C:H/I:H/A:HIf an attacker is able to intercept certain requests to the Kubelet, they can send a redirect response that may be followed by a client using the credentials from the original request. This can lead to compromise of other nodes.If multiple clusters share the same certificate authority trusted by the client, and the same authentication credentials, this vulnerability may allow an attacker to redirect the client to another cluster. In this configuration, this vulnerability should be considered High severity.Am I vulnerable?You are only affected by this vulnerability if you treat the node as a security boundary, or if clusters share certificate authorities and authentication credentials.Note that this vulnerability requires an attacker to first compromise a node through separate means.","affected_version":[{"from":"1.18.0","to":"1.18.5"},{"from":"1.17.0","to":"1.17.8"},{"from":"1.16.0","to":"1.16.12"},{"from":"1.16.0","to":"1.16.0"}],"fixed_version":[{"fixed":"1.18.6"},{"fixed":"1.17.9"},{"fixed":"1.16.13"}],"urls":["https://github.com/kubernetes/kubernetes/issues/92914","https://www.cve.org/cverecord?id=CVE-2020-8559"],"cvss":"CVSS:3.1/AV:N/AC:H/PR:H/UI:R/S:U/C:H/I:H/A:H","severity":"Medium","score":6.4},{"id":"CVE-2020-8558","created_at":"2020-06-19T18:38:58Z","summary":"Node setting allows for neighboring hosts to bypass localhost boundary","component":"github.com/kubernetes/kubelet/kube-proxy","description":"CVSS Rating:In typical clusters: medium (5.4) CVSS:3.1/AV:A/AC:L/PR:N/UI:N/S:U/C:L/I:L/A:NIn clusters where API server insecure port has not been disabled: high (8.8) CVSS:3.1/AV:A/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:HA security issue was discovered in kube-proxy which allows adjacent hosts to reach TCP and UDP services bound to 127.0.0.1 running on the node or in the node's network namespace. For example, if a cluster administrator runs a TCP service on a node that listens on 127.0.0.1:1234, because of this bug, that service would be potentially reachable by other hosts on the same LAN as the node, or by containers running on the same node as the service. If the example service on port 1234 required no additional authentication (because it assumed that only other localhost processes could reach it), then it could be vulnerable to attacks that make use of this bug.The Kubernetes API Server's default insecure port setting causes the API server to listen on 127.0.0.1:8080 where it will accept requests without authentication. Many Kubernetes installers explicitly disable the API Server's insecure port, but in clusters where it is not disabled, an attacker with access to another system on the same LAN or with control of a container running on the master may be able to reach the API server and execute arbitrary API requests on the cluster. This port is deprecated, and will be removed in Kubernetes v1.20.Am I vulnerable?You may be vulnerable if:","affected_version":[{"from":"1.18.0","to":"1.18.3"},{"from":"1.17.0","to":"1.17.6"},{"from":"0.0.0","to":"1.16.10"}],"fixed_version":[{"fixed":"1.18.4"},{"fixed":"1.17.7"},{"fixed":"1.16.11"}],"urls":["https://github.com/kubernetes/kubernetes/issues/92315","https://www.cve.org/cverecord?id=CVE-2020-8558"],"cvss":"CVSS:3.1/AV:A/AC:L/PR:N/UI:N/S:U/C:L/I:L/A:N","severity":"Medium","score":5.4},{"id":"CVE-2020-8555","created_at":"2020-05-28T16:13:34Z","summary":"Half-Blind SSRF in kube-controller-manager","component":"github.com/kubernetes/kube-controller-manager","description":"CVSS Rating: CVSS:3.0/AV:N/AC:H/PR:L/UI:N/S:C/C:H/I:N/A:NThere exists a Server Side Request Forgery (SSRF) vulnerability in kube-controller-manager that allows certain authorized users to leak up to 500 bytes of arbitrary information from unprotected endpoints within the master's host network (such as link-local or loopback services).An attacker with permissions to create a pod with certain built-in Volume types (GlusterFS, Quobyte, StorageOS, ScaleIO) or permissions to create a StorageClass can cause kube-controller-manager to make GET requests or POST requests without an attacker controlled request body from the master's host network.Am I vulnerable?You may be vulnerable if:","affected_version":[{"from":"1.18.0","to":"1.18.0"},{"from":"1.17.0","to":"1.17.4"},{"from":"1.16.0","to":"1.16.8"},{"from":"0.0.0","to":"1.15.11"}],"fixed_version":[{"fixed":"1.18.1"},{"fixed":"1.17.5"},{"fixed":"1.16.9"},{"fixed":"1.15.12"}],"urls":["https://github.com/kubernetes/kubernetes/issues/91542","https://www.cve.org/cverecord?id=CVE-2020-8555"],"cvss":"CVSS:3.0/AV:N/AC:H/PR:L/UI:N/S:C/C:H/I:N/A:N","severity":"Medium","score":6.3},{"id":"CVE-2020-10749","created_at":"2020-05-27T19:32:29Z","summary":"IPv4 only clusters susceptible to MitM attacks via IPv6 rogue router advertisements","component":"github.com/kubernetes/kubelet","description":"CVSS Rating: CVSS:3.1/AV:N/AC:H/PR:L/UI:N/S:C/C:L/I:L/A:L (6.0 Medium)A cluster configured to use an affected container networking implementation is susceptible to man-in-the-middle (MitM) attacks. By sending “rogue” router advertisements, a malicious container can reconfigure the host to redirect part or all of the IPv6 traffic of the host to the attacker-controlled container. Even if there was no IPv6 traffic before, if the DNS returns A (IPv4) and AAAA (IPv6) records, many HTTP libraries will try to connect via IPv6 first then fallback to IPv4, giving an opportunity to the attacker to respond.Am I vulnerable?Kubernetes itself is not vulnerable. A Kubernetes cluster using an affected networking implementation is vulnerable.Binary releases of the kubelet installed from upstream Kubernetes Community repositories hosted at https://packages.cloud.google.com/ may have also installed the kubernetes-cni package containing the containernetworking CNI plugins, which are affected by CVE-2020-10749.","affected_version":[{"from":"1.18.0","to":"1.18.3"},{"from":"1.17.0","to":"1.17.6"},{"from":"0.0.0","to":"1.16.11"},{"from":"0.8.6","to":"0.8.6"},{"from":"19.03.11","to":"9.03.11"},{"from":"2.6.3","to":"2.6.3"}],"fixed_version":[{"fixed":"1.19.0"},{"fixed":"1.18.4"},{"fixed":"1.17.7"},{"fixed":"1.16.11"},{"fixed":"0.8.6"}],"urls":["https://github.com/kubernetes/kubernetes/issues/91507","https://www.cve.org/cverecord?id=CVE-2020-10749"],"cvss":"CVSS:3.1/AV:N/AC:H/PR:L/UI:N/S:C/C:L/I:L/A:L","severity":"Medium","score":6},{"id":"CVE-2019-11254","created_at":"2020-03-26T18:55:26Z","summary":"kube-apiserver Denial of Service vulnerability from malicious YAML payloads","component":"github.com/kubernetes/kube-apiserver","description":"CVE-2019-11254 is a denial of service vulnerability in the kube-apiserver, allowing authorized users sending malicious YAML payloads to cause kube-apiserver to consume excessive CPU cycles while parsing YAML.The issue was discovered via the fuzz test kubernetes/kubernetes#83750.Affected components: Kubernetes API server","affected_version":[{"from":"1.15.9","to":"1.15.10"},{"from":"1.16.0","to":"1.16.8"},{"from":"1.17.0","to":"1.17.3"}],"urls":["https://github.com/kubernetes/kubernetes/issues/89535","https://www.cve.org/cverecord?id=CVE-2019-11254"]},{"id":"CVE-2020-8552","created_at":"2020-03-23T18:35:34Z","summary":"apiserver DoS (oom)","component":"github.com/kubernetes/kube-apiserver","description":"CVSS Rating: CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L (Medium)The Kubernetes API server has been found to be vulnerable to a denial of service attack via authorized API requests.Am I vulnerable?If an attacker that can make an authorized resource request to an unpatched API server (see below), then you are vulnerable to this. Prior to v1.14, this was possible via unauthenticated requests by default.","affected_version":[{"from":"1.17.0","to":"1.17.2"},{"from":"1.16.0","to":"1.16.6"},{"from":"0.0.0","to":"1.15.10"}],"fixed_version":[{"fixed":"1.17.3"},{"fixed":"1.16.7"},{"fixed":"1.15.10"}],"urls":["https://github.com/kubernetes/kubernetes/issues/89378","https://www.cve.org/cverecord?id=CVE-2020-8552"],"cvss":"CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L","severity":"Medium","score":5.3},{"id":"CVE-2020-8551","created_at":"2020-03-23T18:34:40Z","summary":"Kubelet DoS via API","component":"github.com/kubernetes/kubelet","description":"CVSS Rating: CVSS:3.0/AV:A/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L (Medium)The Kubelet has been found to be vulnerable to a denial of service attack via the kubelet API, including the unauthenticated HTTP read-only API typically served on port 10255, and the authenticated HTTPS API typically served on port 10250.Am I vulnerable?If an attacker can make a request to an unpatched kubelet, then you may be vulnerable to this.","affected_version":[{"from":"1.17.0","to":"1.17.2"},{"from":"1.16.0","to":"1.16.6"},{"from":"1.15.0","to":"1.15.9"}],"fixed_version":[{"fixed":"1.17.3"},{"fixed":"1.16.7"},{"fixed":"1.15.10"}],"urls":["https://github.com/kubernetes/kubernetes/issues/89377","https://www.cve.org/cverecord?id=CVE-2020-8551"],"cvss":"CVSS:3.0/AV:A/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L","severity":"Medium","score":4.3},{"id":"CVE-2019-11251","created_at":"2020-02-03T15:12:22Z","summary":"kubectl cp symlink vulnerability","component":"github.com/kubernetes/kubectl","description":"A security issue was discovered in kubectl versions v1.13.10, v1.14.6, and v1.15.3. The issue is of a medium severity and upgrading of kubectl is encouraged to fix the vulnerability.Am I vulnerable?Run kubectl version --client and if it returns versions v1.13.10, v1.14.6, and v1.15.3, you are running a vulnerable version.How do I upgrade?Follow installation instructions hereVulnerability DetailsThe details for this vulnerability are very similar to CVE-2019-1002101 and CVE-2019-11246.A vulnerability has been discovered in kubectl cp that allows a combination of two symlinks to copy a file outside of its destination directory. This could be used to allow an attacker to place a nefarious file using a symlink, outside of the destination tree.This issue is filed as CVE-2019-11251.Two fixes were formulated, one fix to remove symlink support going forwards and a fix with cherry picks made to ensure backwards compatibility.See https://github.com/kubernetes/kubernetes/pull/82143 for the primary fix in v1.16.0 which removes the support of symlinks in kubectl cp. After version 1.16.0, symlink support with kubectl cp is removed, it is recommended instead to use a combination of exec+tar.A second fix has been made to 1.15.4 and backported to 1.14.7 and 1.13.11. This changes the kubectl cp un-tar symlink logic, by unpacking the symlinks after all the regular files have been unpacked. This then guarantees that a file can’t be written through a symlink.See https://github.com/kubernetes/kubernetes/pull/82384 for the fix to version 1.15.4. The following Cherry picks were made from this fix to earlier versions of v1.14.7 and v1.13.11:See https://github.com/kubernetes/kubernetes/pull/82502 for version 1.14.7See https://github.com/kubernetes/kubernetes/pull/82503 for version 1.13.11Thank you to Erik Sjölund (@eriksjolund) for discovering this issue, Tim Allclair and Maciej Szulik for both fixes and the patch release managers for including the fix in their releases./close","urls":["https://github.com/kubernetes/kubernetes/issues/87773","https://www.cve.org/cverecord?id=CVE-2019-11251"]},{"id":"CVE-2018-1002102","created_at":"2019-12-03T22:58:37Z","summary":"Unvalidated redirect","component":"github.com/kubernetes/kubelet","description":"CVSS Rating: CVSS:3.0/AV:N/AC:H/PR:H/UI:R/S:C/C:L/I:N/A:N/E:F (Low)An attacker-controlled Kubelet can return an arbitrary redirect when responding to certain apiserver requests. Impacted kube-apiservers will follow the redirect as a GET request with client-cert credentials for authenticating to the Kubelet.Am I vulnerable?Kubernetes API servers with the StreamingProxyRedirects feature enabled AND without the ValidateProxyRedirects feature are affected.API servers using SSH tunnels (--ssh-user / --ssh-keyfile) are not affected.Using the default feature gate values, kube-apiserver versions before v1.14 are affected.How do I mitigate this vulnerability?For Kubernetes versions \u003e= v1.10.0, the ValidateProxyRedirects can be manually enabled with the kube-apiserver flag --feature-gates=ValidateProxyRedirects=true.Fix impactThe ValidateProxyRedirects feature will cause the kube-apiserver to check that redirects go to the same host. If nodes are configured to respond to CRI streaming requests on a different host interface than what the apiserver makes requests on (only the case if not using the built-in dockershim \u0026 setting the kubelet flag --redirect-container-streaming=true), then these requests will be broken. In that case, the feature can be temporarily disabled until the node configuration is corrected. We suggest setting --redirect-container-streaming=false on the kubelet to avoid issues.","urls":["https://github.com/kubernetes/kubernetes/issues/85867","https://www.cve.org/cverecord?id=CVE-2018-1002102"],"cvss":"CVSS:3.0/AV:N/AC:H/PR:H/UI:R/S:C/C:L/I:N/A:N","severity":"Low","score":2.6},{"id":"CVE-2019-11255","created_at":"2019-11-13T20:57:31Z","summary":"CSI volume snapshot, cloning and resizing features can result in unauthorized volume data access or mutation","component":"github.com/kubernetes/kube-controller-manager","description":"Am I vulnerable?CSI snapshot, cloning and resizing features are affected. Prior to Kubernetes 1.16, these features were all alpha and disabled by default. Starting in Kubernetes 1.16, CSI cloning and resizing features are beta and enabled by default.These features also require CSI drivers to be installed in a Kubernetes cluster and the CSI driver also has to support those features. An unofficial list of CSI drivers and their supported features is available here, however it is best to check with the CSI driver vendor for the latest information.Check if you have the following Kubernetes feature gates enabled:Check if you are using CSI drivers in your cluster. If so, the following command’s output will be non-empty:Then, check the CSI driver’s pod specifications to see if they are using the following vulnerable versions of sidecars:An example query:Note that the exact container image name may vary across CSI driver vendors. It is recommended to inspect the Pod specifications directly.How do I mitigate the vulnerability?As a short term mitigation, disable the VolumeSnapshotDataSource, ExpandCSIVolumes, and VolumePVCDataSource Kubernetes feature gates in kube-apiserver and kube-controller-manager. This will cause new PersistentVolumeClaims to be provisioned ignoring the DataSource and resizing requests will also be ignored. Note that this will cause new PVCs that are intended to be provisioned from a snapshot or clone to instead provision a blank disk.Also, to disable taking volume snapshots, either remove the external-snapshotter sidecar from any CSI drivers or revoke the CSI driver’s RBAC permissions on the snapshot.storage.k8s.io API group.Longer term, upgrade your CSI driver with patched versions of the affected sidecars. Fixes are available in the following sidecar versions:external-provisioner: v0.4.3v1.0.2v1.2.2v1.3.1v1.4.0external-snapshotter:v0.4.2v1.0.2v1.2.2external-resizerv0.3.0Fixes for each of the sidecars can be tracked by:https://github.com/kubernetes-csi/external-provisioner/issues/380 https://github.com/kubernetes-csi/external-snapshotter/issues/193 https://github.com/kubernetes-csi/external-resizer/issues/63How do I upgrade?Check with your CSI driver vendor for upgrade instructions. No Kubernetes control plane or node upgrades are required unless the CSI driver is bundled into the Kubernetes distribution.Vulnerability detailsThere are two different vulnerabilities impacting the same features.When PersistentVolumeClaim and PersistentVolume objects are bound, they have bidirectional references to each other. When dereferencing a PersistentVolumeClaim to get a PersistentVolume, the impacted sidecar controllers were not validating that the PersistentVolume referenced back to the same PersistentVolumeClaim, potentially operating on unauthorized PersistentVolumes for snapshot, cloning and resizing operations.A similar issue exists for VolumeSnapshot and VolumeSnapshotContent objects when creating a new PersistentVolumeClaim from a snapshot.The second issue is related to the property that CSI volume and snapshot ids are only required to be unique within a single CSI driver. Impacted sidecar controllers were not validating that the requested source VolumeSnapshot or PersistentVolumeClaim specified were from the same driver processing the request, potentially operating on unauthorized volumes during snapshot, restore from snapshot, or cloning operations.","urls":["https://github.com/kubernetes/kubernetes/issues/85233","https://www.cve.org/cverecord?id=CVE-2019-11255"]},{"id":"CVE-2019-11253","created_at":"2019-09-27T16:53:31Z","summary":"Kubernetes API Server JSON/YAML parsing vulnerable to resource exhaustion attack","component":"github.com/kubernetes/Kubernetes","description":"CVE-2019-11253 is a denial of service vulnerability in the kube-apiserver, allowing authorized users sending malicious YAML or JSON payloads to cause kube-apiserver to consume excessive CPU or memory, potentially crashing and becoming unavailable. This vulnerability has been given an initial severity of High, with a score of 7.5 (CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H).Prior to v1.14.0, default RBAC policy authorized anonymous users to submit requests that could trigger this vulnerability. Clusters upgraded from a version prior to v1.14.0 keep the more permissive policy by default for backwards compatibility. See the mitigation section below for instructions on how to install the more restrictive v1.14+ policy.","affected_version":[{"from":"1.0.0","to":"1.0.0"},{"from":"1.13.0","to":"1.13.12"},{"from":"1.14.0","to":"1.14.8"},{"from":"1.15.0","to":"1.15.5"},{"from":"1.16.0","to":"1.16.2"},{"from":"1.14.0","to":"1.14.0"},{"from":"1.14.0","to":"1.14.0"},{"from":"1.16.0","to":"1.16.0"},{"from":"1.15.0","to":"1.15.0"},{"from":"1.16.0","to":"1.16.0"},{"from":"1.16.0","to":"1.16.0"}],"urls":["https://github.com/kubernetes/kubernetes/issues/83253","https://www.cve.org/cverecord?id=CVE-2019-11253"],"cvss":"CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H","severity":"High","score":7.5},{"id":"CVE-2019-11250","created_at":"2019-08-08T02:03:04Z","summary":"Bearer tokens are revealed in logs","component":"github.com/kubernetes/kube-apiserver","description":"This issue was reported in the Kubernetes Security Audit ReportDescription Kubernetes requires an authentication mechanism to enforce users’ privileges. One method of authentication, bearer tokens, are opaque strings used to associate a user with their having successfully authenticated previously. Any user with possession of this token may masquerade as the original user (the “bearer”) without further authentication.Within Kubernetes, the bearer token is captured within the hyperkube kube-apiserver system logs at high verbosity levels (--v 10). A malicious user with access to the system logs on such a system could masquerade as any user who has previously logged into the system.Exploit Scenario Alice logs into a Kubernetes cluster and is issued a Bearer token. The system logs her token. Eve, who has access to the logs but not the production Kubernetes cluster, replays Alice’s Bearer token, and can masquerade as Alice to the cluster.Recommendation Short term, remove the Bearer token from the log. Do not log any authentication credentials within the system, including tokens, private keys, or passwords that may be used to authenticate to the production Kubernetes cluster, regardless of the logging level.Long term, either implement policies that enforce code review to ensure that sensitive data is not exposed in logs, or implement logging filters that check for sensitive data and remove it prior to outputting the log. In either case, ensure that sensitive data cannot be trivially stored in logs.Anything else we need to know?:See #81146 for current status of all issues created from these findings.The vendor gave this issue an ID of TOB-K8S-001 and it was finding 6 of the report.The vendor considers this issue Medium Severity.To view the original finding, begin on page 31 of the Kubernetes Security Review ReportEnvironment:","urls":["https://github.com/kubernetes/kubernetes/issues/81114","https://www.cve.org/cverecord?id=CVE-2019-11250"]},{"id":"CVE-2019-11248","created_at":"2019-08-06T14:34:33Z","summary":"/debug/pprof exposed on kubelet's healthz port","component":"github.com/kubernetes/kubelet","description":"The debugging endpoint /debug/pprof is exposed over the unauthenticated Kubelet healthz port. Versions prior to 1.15.0, 1.14.4, 1.13.8, and 1.12.10 are affected. The issue is of medium severity, but not exposed by the default configuration. If you are exposed we recommend upgrading to at least one of the versions listed.Am I vulnerable? By default, the Kubelet exposes unauthenticated healthz endpoints on port :10248, but only over localhost. If your nodes are using a non-localhost healthzBindAddress (--health-bind-address), and an older version, you may be vulnerable. If your nodes are using the default localhost healthzBindAddress, it is only exposed to pods or processes running in the host network namespace.Run kubectl get nodes to see whether nodes are running a vulnerable version.Run kubectl get --raw /api/v1/nodes/${NODE_NAME}/proxy/configz to check whether the \"healthzBindAddress\" is non-local.How do I mitigate the vulnerability?https://github.com/kubernetes/kubernetes/pull/79184 fixed in 1.12.10https://github.com/kubernetes/kubernetes/pull/79183 fixed in 1.13.8https://github.com/kubernetes/kubernetes/pull/79182 fixed in 1.14.4https://github.com/kubernetes/kubernetes/pull/78313 fixed in 1.15.0Vulnerability Details The go pprof endpoint is exposed over the Kubelet's healthz port. This debugging endpoint can potentially leak sensitive information such as internal Kubelet memory addresses and configuration, or for limited denial of service.Thanks to Jordan Zebor of F5 Networks for reporting this problem./area security/close","urls":["https://github.com/kubernetes/kubernetes/issues/81023","https://www.cve.org/cverecord?id=CVE-2019-11248"]},{"id":"CVE-2019-11249","created_at":"2019-08-05T12:44:23Z","summary":"Incomplete fixes for CVE-2019-1002101 and CVE-2019-11246, kubectl cp potential directory traversal","component":"github.com/kubernetes/kubectl","description":"CVSS:3.0/AV:N/AC:H/PR:L/UI:R/S:U/C:N/I:H/A:NA third issue was discovered with the Kubernetes kubectl cp command that could enable a directory traversal such that a malicious container could replace or create files on a user’s workstation. The vulnerability is a client-side defect and requires user interaction to be exploited.Vulnerable versions: Kubernetes 1.0.x-1.12.xKubernetes 1.13.0-1.13.8Kubernetes 1.14.0-1.14.4Kubernetes 1.15.0-1.15.1Vulnerable configurations: All kubectl clients running a vulnerable version and using the cp operation.Vulnerability impact: A malicious user can potentially create or overwrite files outside of the destination directory of the kubectl cp operation.Mitigations prior to upgrading: Avoid using kubectl cp with any untrusted workloads.","fixed_version":[{"fixed":"1.13.9"},{"fixed":"1.14.5"},{"fixed":"1.15.2"}],"urls":["https://github.com/kubernetes/kubernetes/issues/80984","https://www.cve.org/cverecord?id=CVE-2019-11249"],"cvss":"CVSS:3.0/AV:N/AC:H/PR:L/UI:R/S:U/C:N/I:H/A:N","severity":"Medium","score":4.8},{"id":"CVE-2019-11247","created_at":"2019-08-05T12:44:08Z","summary":"API server allows access to custom resources via wrong scope","component":"github.com/kubernetes/","description":"CVSS:3.0/AV:N/AC:H/PR:L/UI:N/S:U/C:L/I:L/A:LThe API server mistakenly allows access to a cluster-scoped custom resource if the request is made as if the resource were namespaced. Authorizations for the resource accessed in this manner are enforced using roles and role bindings within the namespace, meaning that a user with access only to a resource in one namespace could create, view update or delete the cluster-scoped resource (according to their namespace role privileges).Vulnerable versions: Kubernetes 1.7.x-1.12.xKubernetes 1.13.0-1.13.8Kubernetes 1.14.0-1.14.4Kubernetes 1.15.0-1.15.1Vulnerable configurations: All clusters that have rolebindings to roles and clusterroles that include authorization rules for cluster-scoped custom resources.Vulnerability impact: A user with access to custom resources in a single namespace can access custom resources with cluster scope.Mitigations prior to upgrading: To mitigate, remove authorization rules that grant access to cluster-scoped resources within namespaces. For example, RBAC roles and clusterroles intended to be referenced by namespaced rolebindings should not grant access to resources:[*], apiGroups:[*], or grant access to cluster-scoped custom resources.","fixed_version":[{"fixed":"1.13.9"},{"fixed":"1.14.5"},{"fixed":"1.15.2"}],"urls":["https://github.com/kubernetes/kubernetes/issues/80983","https://www.cve.org/cverecord?id=CVE-2019-11247"],"cvss":"CVSS:3.0/AV:N/AC:H/PR:L/UI:N/S:U/C:L/I:L/A:L","severity":"Medium","score":5},{"id":"CVE-2019-11245","created_at":"2019-05-24T16:14:49Z","summary":"container uid changes to root after first restart or if image is already pulled to the node","component":"github.com/kubernetes/versions","description":"CVSS:3.0/AV:L/AC:H/PR:N/UI:N/S:U/C:L/I:L/A:L, 4.9 (medium)In kubelet v1.13.6 and v1.14.2, containers for pods that do not specify an explicit runAsUser attempt to run as uid 0 (root) on container restart, or if the image was previously pulled to the node. If the pod specified mustRunAsNonRoot: true, the kubelet will refuse to start the container as root. If the pod did not specify mustRunAsNonRoot: true, the kubelet will run the container as uid 0.CVE-2019-11245 will be fixed in the following Kubernetes releases:Fixed by #78261 in masterAffected components:","affected_version":[{"from":"1.13.6","to":"1.13.6"},{"from":"1.14.2","to":"1.14.2"},{"from":"1.13.6","to":"1.14.2"},{"from":"1.14.1","to":"1.13.5"},{"from":"1.14.1","to":"1.14.1"},{"from":"1.14.2","to":"1.14.2"},{"from":"1.1.0","to":"1.1.0"},{"from":"1.14.2","to":"8.09.6"},{"from":"1.14.2","to":"1.14.2"},{"from":"1.14.2","to":"1.14.2"},{"from":"1.13.5","to":"1.14.2"},{"from":"1.1.0","to":"1.1.0"}],"urls":["https://github.com/kubernetes/kubernetes/issues/78308","https://www.cve.org/cverecord?id=CVE-2019-11245"],"cvss":"CVSS:3.0/AV:L/AC:H/PR:N/UI:N/S:U/C:L/I:L/A:L","severity":"Medium","score":4.9},{"id":"CVE-2019-11243","created_at":"2019-04-18T21:31:53Z","summary":"rest.AnonymousClientConfig() does not remove the serviceaccount credentials from config created by rest.InClusterConfig()","component":"github.com/kubernetes/","description":"CVSS:3.0/AV:N/AC:H/PR:L/UI:N/S:U/C:L/I:N/A:NThe rest.AnonymousClientConfig() method returns a copy of the provided config, with credentials removed (bearer token, username/password, and client certificate/key data).In the following versions, rest.AnonymousClientConfig() did not effectively clear service account credentials loaded using rest.InClusterConfig():What is the impact?How was the issue fixed?How do I resolve the issue?Thanks to Oleg Bulatov of Red Hat for reporting this issue./area security/kind bug/sig auth/sig api-machinery/assign/close","urls":["https://github.com/kubernetes/kubernetes/issues/76797","https://www.cve.org/cverecord?id=CVE-2019-11243"],"cvss":"CVSS:3.0/AV:N/AC:H/PR:L/UI:N/S:U/C:L/I:N/A:N","severity":"Low","score":3.1},{"id":"CVE-2019-11244","created_at":"2019-04-16T20:14:25Z","summary":"`kubectl:-http-cache=\u003cworld-accessible dir\u003e` creates world-writeable cached schema files","component":"github.com/kubernetes/kubectl","description":"In kubectl v1.8.0+, schema info is cached in the location specified by --cache-dir (defaulting to $HOME/.kube/http-cache), written with world-writeable permissions (rw-rw-rw-).If --cache-dir is specified and pointed at a different location accessible to other users/groups, the written files may be modified by other users/groups and disrupt the kubectl invocation.CVSS score: CVSS:3.0/AV:L/AC:H/PR:L/UI:R/S:U/C:L/I:L/A:N (3.3, low)What versions are affected? kubectl v1.8.0+What configurations are affected? Invocations that point --cache-dir at world-writeable locationsImpact Malformed responses written to the cache directory can disrupt the kubectl invocationWorkaround Use the default --http-cache location in the $HOME directory or point it at a directory that is only accessible to desired users/groups.(original description follows) ====What happened: The files inside of \".kube/http-cache\" are world writeable (rw-rw-rw-). While the default for these files appears to be the home directory, using the \"--cache-dir\" flag could put these files into a place where world writeable files would allow any user / process to modify the cache files. Modification of the cache files could influence the kubectl utility in a negative way for other users.What you expected to happen: Apply stricter file permissions to the http-cache files.How to reproduce it (as minimally and precisely as possible): Run any generic kubectl command which is successful and then list the cache directory ~/.kube/http-cache/*$ kubectl get pods --all-namespaces$ ls -la ~/.kube/http-cache/*Anything else we need to know?: I estimate this is a low severity security issue with a CVSS score of \"3.3 / CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:L/A:N\" - https://www.first.org/cvss/calculator/3.0#CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:L/A:NEnvironment: LinuxKubernetes version (use kubectl version):Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.6\", GitCommit:\"ab91afd7062d4240e95e51ac00a18bd58fddd365\", GitTreeState:\"clean\", BuildDate:\"2019-02-26T12:49:28Z\", GoVersion:\"go1.10.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}Server Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.6\", GitCommit:\"ab91afd7062d4240e95e51ac00a18bd58fddd365\", GitTreeState:\"clean\", BuildDate:\"2019-02-26T12:49:28Z\", GoVersion:\"go1.10.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}Cloud provider or hardware configuration: AWS. Running kube api server in hyperkube.OS (e.g: cat /etc/os-release):NAME=\"CentOS Linux\"VERSION=\"7.1808 (Core)\"ID=\"centos\"ID_LIKE=\"rhel fedora\"VERSION_ID=\"7\"PRETTY_NAME=\"CentOS Linux 7.1808 (Core)\"ANSI_COLOR=\"0;31\"CPE_NAME=\"cpe:/o:centos:centos:7\"HOME_URL=\"https://www.centos.org/\"BUG_REPORT_URL=\"https://bugs.centos.org/\"CENTOS_MANTISBT_PROJECT=\"CentOS-7\"CENTOS_MANTISBT_PROJECT_VERSION=\"7\"REDHAT_SUPPORT_PRODUCT=\"centos\"REDHAT_SUPPORT_PRODUCT_VERSION=\"7\"OSTREE_VERSION=7.1808Kernel (e.g. uname -a): Linux hackit.internal 3.10.0-862.11.6.el7.x86_64 #1 SMP Tue Aug 14 21:49:04 UTC 2018 x86_64 x86_64 x86_64 GNU/LinuxInstall tools: Manual installation.Others: n/a","urls":["https://github.com/kubernetes/kubernetes/issues/76676","https://www.cve.org/cverecord?id=CVE-2019-11244"],"cvss":"CVSS:3.0/AV:L/AC:H/PR:L/UI:R/S:U/C:L/I:L/A:N","severity":"Low","score":3.3},{"id":"CVE-2019-1002100","created_at":"2019-02-25T19:39:09Z","summary":"json-patch requests can exhaust apiserver resources","component":"github.com/kubernetes/Kubernetes","description":"CVSS:3.0/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H](https://www.first.org/cvss/calculator/3.0#CVSS:3.0/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H) (6.5, medium)Users that are authorized to make patch requests to the Kubernetes API Server can send a specially crafted patch of type “json-patch” (e.g. kubectl patch --type json or \"Content-Type: application/json-patch+json\") that consumes excessive resources while processing, causing a Denial of Service on the API Server.Thanks to Carl Henrik Lunde for reporting this problem.CVE-2019-1002100 is fixed in the following Kubernetes releases:Affected components:","affected_version":[{"from":"1.11.0","to":"1.11.8"},{"from":"1.12.0","to":"1.12.6"},{"from":"1.13.0","to":"1.13.4"}],"urls":["https://github.com/kubernetes/kubernetes/issues/74534","https://www.cve.org/cverecord?id=CVE-2019-1002100"],"cvss":"CVSS:3.0/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H","severity":"Medium","score":6.5},{"id":"CVE-2018-1002105","created_at":"2018-11-26T11:07:36Z","summary":"proxy request handling in kube-apiserver can leave vulnerable TCP connections","component":"github.com/kubernetes/Kubernetes","description":"CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H (9.8, critical)With a specially crafted request, users that are authorized to establish a connection through the Kubernetes API server to a backend server can then send arbitrary requests over the same connection directly to that backend, authenticated with the Kubernetes API server’s TLS credentials used to establish the backend connection.Thanks to Darren Shepherd for reporting this problem.CVE-2018-1002105 is fixed in the following Kubernetes releases:Affected components:","affected_version":[{"from":"1.10.0","to":"1.10.11"},{"from":"1.11.0","to":"1.11.5"},{"from":"1.12.0","to":"1.12.3"}],"urls":["https://github.com/kubernetes/kubernetes/issues/71411","https://www.cve.org/cverecord?id=CVE-2018-1002105"],"cvss":"CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H","severity":"Critical","score":9.8},{"id":"CVE-2018-1002101","created_at":"2018-07-03T08:06:15Z","summary":"smb mount security issue","component":"github.com/kubernetes/","description":"This issue is tracked under CVE-2018-1002101Is this a BUG REPORT or FEATURE REQUEST?:/kind bugWhat happened:user PowerShell Environment Variables to store user input string to prevent command line injection, the env var in PowerShell would be taken as literal values and not as executable vulnerable code, this kind of fix is common for command line injection issue (called: parameterized way)What you expected to happen:How to reproduce it (as minimally and precisely as possible):Anything else we need to know?:Environment:/sig windows/sig storage/assign","urls":["https://github.com/kubernetes/kubernetes/issues/65750","https://www.cve.org/cverecord?id=CVE-2018-1002101"]},{"id":"CVE-2018-1002100","created_at":"2018-03-16T19:24:46Z","summary":"Kubectl copy doesn't check for paths outside of it's destination directory.","component":"github.com/kubernetes/kubectl","description":"Is this a BUG REPORT or FEATURE REQUEST?: Bug/kind bugWhat happened:kubectl cp :/some/remote/dir /some/local/dirIf the container returns a malformed tarfile with paths like:'/some/remote/dir/../../../../tmp/foo' kubectl writes this to /tmp/foo instead of /some/local/dir/tmp/fooWhat you expected to happen:I expect kubectl to clean up the path and write to /some/local/dir/tmp/fooNotes Original credit to @hansmi (Michael Hanselmann) for originally reporting the bug.Tracked as  CVE-2018-1002100","urls":["https://github.com/kubernetes/kubernetes/issues/61297","https://www.cve.org/cverecord?id=CVE-2018-1002100"]},{"id":"CVE-2017-1002102","created_at":"2018-03-05T20:55:20Z","summary":"atomic writer volume handling allows arbitrary file deletion in host filesystem","component":"github.com/kubernetes/kubelet","description":"CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:L/A:HThis vulnerability allows containers using a secret, configMap, projected or downwardAPI volume to trigger deletion of arbitrary files and directories on the nodes where they are running.Thanks to Joel Smith of Red Hat for reporting this problem.Vulnerable versions:Vulnerable configurations:Vulnerability impact: A malicious container running in a pod with a secret, configMap, downwardAPI or projected volume mounted (including auto-added service account token mounts) can cause the Kubelet to remove any file or directory on the host filesystem.Mitigations prior to upgrading: Do not allow containers to be run with secret, configMap, downwardAPI and projected volumes (note that this prevents use of service account tokens in pods, and requires use of  automountServiceAccountToken: false)","fixed_version":[{"fixed":"1.7.14"},{"fixed":"1.8.9"},{"fixed":"1.9.4"},{"fixed":"1.10.0"},{"fixed":"1.10.0"}],"urls":["https://github.com/kubernetes/kubernetes/issues/60814","https://www.cve.org/cverecord?id=CVE-2017-1002102"],"cvss":"CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:L/A:H","severity":"Medium","score":6.1},{"id":"CVE-2017-1002101","created_at":"2018-03-05T20:53:58Z","summary":"subpath volume mount handling allows arbitrary file access in host filesystem","component":"github.com/kubernetes/","description":"CVSS:3.0/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:HThis vulnerability allows containers using subpath volume mounts with any volume type (including non-privileged pods, subject to file permissions) to access files/directories outside of the volume, including the host’s filesystem.Thanks to Maxim Ivanov for reporting this problem.Vulnerable versions:Vulnerable configurations:Vulnerability impact: A specially crafted pod spec combined with malicious container behavior can allow read/write access to arbitrary files outside volumes specified in the pod, including the host’s filesystem. This can be accomplished with any volume type, including emptyDir, and can be accomplished with a non-privileged pod (subject to file permissions).Mitigations prior to upgrading: Prevent untrusted users from creating pods (and pod-creating objects like deployments, replicasets, etc), or disable all volume types with PodSecurityPolicy (note that this prevents use of service account tokens in pods, and requires use of  automountServiceAccountToken: false)","fixed_version":[{"fixed":"1.7.14"},{"fixed":"1.8.9"},{"fixed":"1.9.4"},{"fixed":"1.10.0"},{"fixed":"1.10.0"}],"urls":["https://github.com/kubernetes/kubernetes/issues/60813","https://www.cve.org/cverecord?id=CVE-2017-1002101"],"cvss":"CVSS:3.0/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H","severity":"High","score":8.8},{"id":"CVE-2017-1002100","created_at":"2017-06-15T18:59:13Z","summary":"Azure PV should be Private scope not Container scope","component":"github.com/kubernetes/","urls":["https://github.com/kubernetes/kubernetes/issues/47611","https://www.cve.org/cverecord?id=CVE-2017-1002100"]},{"id":"CVE-2017-1000056","created_at":"2017-03-21T15:22:29Z","summary":"PodSecurityPolicy admission plugin authorizes incorrectly","component":"github.com/kubernetes/","description":"A PodSecurityPolicy admission plugin vulnerability allows users to make use of any PodSecurityPolicy object, even ones they are not authorized to use.CVE: CVE-2017-1000056Who is affected? Only Kubernetes 1.5.0-1.5.4 installations that do all of the following:kubeadm and GKE do not allow enabling PodSecurityPolicy in 1.5, so are not affected by this vulnerability.kube-up.sh and kops do not enable PodSecurityPolicy by default, so are not affected by this vulnerability. A modified kube-up.sh or kops deployment could have enabled it.What is the impact? A user that is authorized to create pods can make use of any existing PodSecurityPolicy, even ones they are not authorized to use.How can I mitigate this prior to installing 1.5.5?","urls":["https://github.com/kubernetes/kubernetes/issues/43459","https://www.cve.org/cverecord?id=CVE-2017-1000056"]}]